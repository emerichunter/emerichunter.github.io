---
layout: post
Title: Test of PostgreSQL's client failover
Draft: true
published: true
---


Today I bring you a test and some measurments of PostgreSQL 10 client replication.

So some time ago i came across this new feature everyone was so excited about. 
At first I didn't realize the implications. 
But then I was doing some testing for automated failover.
Numerous clienst sent requests to me, so i started to check out the options available.

A quick test with Stolon soon ruled it out : not compatible with my home made deployment tool and so was Patroni.
Although both seemed to work fine, none was what I was looking  for.
Of course pacemaker/corosync was considered but failover to a different site was an issue. 

Last option was REPMGR.
So it took some time to figure things out.
Eventually, all this good work came to fruition.
My 3 nodes cluster (+ a witness) was working alright, eventhough network failures were badly handled.
Well I know this tool is only supposed to work on database failure but still.

I was planning on how to measure failover time when a colleague of mine reminded me of this new feature from PostgreSQL libpq.

## Measuring write loss over time

So here is the link to the setup I used for measurement [monitoring_replication](https://github.com/emerichunter/monitoring_replication).
Every step is explained in the README.

I made this to know how much writes could be lost.

Here is a selected extract of repmgr.conf.

~~~~
cluster=test_repmgr
node=4
node_name=node4
conninfo='port=5432 host=server2 user=repmgr dbname=repmgr'
failover=automatic
promote_command='repmgr standby promote -f /etc/repmgr.conf --log-to-file'
follow_command='repmgr standby follow -f /etc/repmgr.conf --log-to-file'
                 # the server's hostname or another identifier unambiguously
                 # associated with the server to avoid confusion
logfile='/var/log/repmgr/repmgr-9.6.log'
pg_bindir=/usr/pgsql-9.6/bin/

master_response_timeout=5
reconnect_attempts=3
reconnect_interval=5
retry_promote_interval_secs=10

~~~~

So we are expecting 5 + 3*5 =20 seconds of downtime.

## Let's fill the WAL FS

First failover on the standby when Wal FS is filled up. Here is an extract of the log file. Notice the timestamp. 21 seconds for the standby to be promoted.

~~~~

log node 2 (standby failover)
[2017-07-13 10:29:25] [ERROR] connection to database failed: could not connect to server: Connection refused
        Is the server running on host "server1" (172.33.1.45) and accepting
        TCP/IP connections on port 5432?

[2017-07-13 10:29:25] [ERROR] unable to connect to upstream node: could not connect to server: Connection refused
        Is the server running on host "server1" (172.33.1..45) and accepting
        TCP/IP connections on port 5432?

[2017-07-13 10:29:25] [ERROR] connection to database failed: could not connect to server: Connection refused
        Is the server running on host "server1" (172.33.1..45) and accepting
        TCP/IP connections on port 5432?

[2017-07-13 10:29:25] [WARNING] connection to master has been lost, trying to recover... 15 seconds before failover decision
[2017-07-13 10:29:30] [WARNING] connection to master has been lost, trying to recover... 10 seconds before failover decision
[2017-07-13 10:29:35] [WARNING] connection to master has been lost, trying to recover... 5 seconds before failover decision
[2017-07-13 10:29:40] [ERROR] unable to reconnect to master (timeout 5 seconds)...
[2017-07-13 10:29:45] [NOTICE] this node is the best candidate to be the new master, promoting...
[2017-07-13 10:29:46] [NOTICE] Redirecting logging output to '/var/log/repmgr/repmgr-9.6.log'
[2017-07-13 10:29:46] [ERROR] connection to database failed: could not connect to server: Connection refused
        Is the server running on host "server1" (172.33.1.45) and accepting
        TCP/IP connections on port 5432?

[2017-07-13 10:29:46] [ERROR] connection to database failed: could not connect to server: Connection refused
        Is the server running on host "server2" (1172.33.1.99) and accepting
        TCP/IP connections on port 5432?

[2017-07-13 10:29:46] [NOTICE] promoting standby
[2017-07-13 10:29:46] [NOTICE] promoting server using '/usr/pgsql-9.6/bin/pg_ctl -D /appli/postgres/test_repmgr/9.6/data promote'
[2017-07-13 10:29:48] [NOTICE] STANDBY PROMOTE successful

~~~~


And then the time measurement of writes loss during client failover to the newly elected master. (extract)

~~~~
|pk	  |  time                           |
|-----|:-------------------------------:|
|5876 |	2017-07-13 10:29:24.221169+02   |
|5877 |	2017-07-13 10:29:24.277367+02   |
|5878 |	2017-07-13 10:29:24.330156+02   |
|5879 |	2017-07-13 10:29:24.38804+02    |
|5880 |	2017-07-13 10:29:24.441046+02   |
|5881 |	2017-07-13 10:29:24.493518+02   |
|5882 |	2017-07-13 10:29:24.545547+02   |
|5883 |	2017-07-13 10:29:24.597415+02   |
|5884 |	2017-07-13 10:29:24.649334+02   |
|5885 |	2017-07-13 10:29:24.701244+02   |
|**5886** |	**2017-07-13 10:29:24.753549+02**   |
|**5911** |	**2017-07-13 10:29:46.685232+02**   |
|5912 |	2017-07-13 10:29:46.832526+02   |
|5913 |	2017-07-13 10:29:46.88604+02    |
|5914 |	2017-07-13 10:29:46.939793+02   |
|5915 |	2017-07-13 10:29:46.997917+02   |
|5916 |	2017-07-13 10:29:47.053968+02   |
|5917 |	2017-07-13 10:29:47.108371+02   |

~~~~

We get writes back on the new master after losing around 21 seconds. 25 INSERTS are missing.

## Conclusion 

It works. It awesome. It's PostgreSQL. 

I'm partial. Of course. 


## Some  more info for transparency 

Here is the full configuration repmgr.conf 


~~~~
###################################################
# Replication Manager sample configuration file
###################################################

# Some configuration items will be set with a default value; this
# is noted for each item. Where no default value is shown, the
# parameter will be treated as empty or false.

# Required configuration items
# ============================
#
# repmgr and repmgrd require these items to be configured:

# Cluster name - this will be used by repmgr to generate its internal
# schema (pattern: "repmgr_{cluster}"); while this name will be quoted
# to preserve case, we recommend using lower case and avoiding whitespace
# to facilitate easier querying of the repmgr views and tables.
#cluster=example_cluster
cluster=test_repmgr
node=4
node_name=node4
conninfo='port=5432 host=PCYYYPFG user=repmgr dbname=repmgr'

# Automatic failover
failover=automatic
promote_command='repmgr standby promote -f /etc/repmgr.conf --log-to-file'
follow_command='repmgr standby follow -f /etc/repmgr.conf --log-to-file'


# Node ID and name
# (Note: we recommend to avoid naming nodes after their initial
#  replication function, as this will cause confusion when e.g.
#  "standby2" is promoted to primary)
#node=2           # a unique integer
#node_name=node2  # an arbitrary (but unique) string; we recommend using
                 # the server's hostname or another identifier unambiguously
                 # associated with the server to avoid confusion

# Database connection information as a conninfo string
# This must be accessible to all servers in the cluster; for details see:
#
#   https://www.postgresql.org/docs/current/static/libpq-connect.html#LIBPQ-CONNSTRING
#
#conninfo='host=192.168.204.104 dbname=repmgr user=repmgr'
#
# If repmgrd is in use, consider explicitly setting `connect_timeout` in the
# conninfo string to determine the length of time which elapses before
# a network connection attempt is abandoned; for details see:
#
#   https://www.postgresql.org/docs/current/static/libpq-connect.html#LIBPQ-CONNECT-CONNECT-TIMEOUT

# Optional configuration items
# ============================

# Replication settings
# ---------------------

# When using cascading replication, a standby can connect to another
# upstream standby node which is specified by setting 'upstream_node'.
# In that case, the upstream node must exist before the new standby
# can be registered. If 'upstream_node' is not set, then the standby
# will connect directly to the primary node.
#upstream_node=1

# use physical replication slots - PostgreSQL 9.4 and later only
# (default: 0)
#use_replication_slots=0

# NOTE: 'max_replication_slots' should be configured for at least the
# number of standbys which will connect to the primary.

# Logging and monitoring settings
# -------------------------------

# Log level: possible values are DEBUG, INFO, NOTICE, WARNING, ERR, ALERT, CRIT or EMERG
# (default: NOTICE)
#loglevel=NOTICE

# Note that logging facility settings will only apply to `repmgrd` by default;
# `repmgr` will always write to STDERR unless the switch `--log-to-file` is
# supplied, in which case it will log to the same destination as `repmgrd`.
# This is mainly intended for those cases when `repmgr` is executed directly
# by `repmgrd`.

# Logging facility: possible values are STDERR or - for Syslog integration - one of LOCAL0, LOCAL1, ..., LOCAL7, USER
# (default: STDERR)
#logfacility=STDERR

# stderr can be redirected to an arbitrary file:
#
logfile='/var/log/repmgr/repmgr-9.6.log'

# event notifications can be passed to an arbitrary external program
# together with the following parameters:
#
#   %n - node ID
#   %e - event type
#   %s - success (1 or 0)
#   %t - timestamp
#   %d - details
#
# the values provided for "%t" and "%d" will probably contain spaces,
# so should be quoted in the provided command configuration, e.g.:
#
#event_notification_command='/path/to/some/script %n %e %s "%t" "%d"'

# By default, all notifications will be passed; the notification types
# can be filtered to explicitly named ones:
#
#event_notifications=master_register,standby_register,witness_create


# Environment/command settings
# ----------------------------

# path to PostgreSQL binary directory (location of pg_ctl, pg_basebackup etc.)
# (if not provided, defaults to system $PATH)
#pg_bindir=/usr/bin/
pg_bindir=/usr/pgsql-9.6/bin/

# Debian/Ubuntu users: you will probably need to set this to the directory
# where `pg_ctl` is located, e.g. /usr/lib/postgresql/9.5/bin/

# service control commands
#
# repmgr provides options to override the default pg_ctl commands
# used to stop, start, restart, reload and promote the PostgreSQL cluster
#
# NOTE: These commands must be runnable on remote nodes as well for switchover
# to function correctly.
#
# If you use sudo, the user repmgr runs as (usually 'postgres')  must have
# passwordless sudo access to execute the command
#
# For example, to use systemd, you may use the following configuration:
#
#    # this is required when running sudo over ssh without -t:
#    Defaults:postgres !requiretty
#    postgres ALL = NOPASSWD: /usr/bin/systemctl stop postgresql-9.5, \
#       /usr/bin/systemctl start postgresql-9.5, \
#       /usr/bin/systemctl restart postgresql-9.5
#
# service_start_command = systemctl start postgresql-9.6
# service_stop_command = systemctl stop postgresql-9.6
# service_restart_command = systemctl restart postgresql-9.6
# service_reload_command = systemctl reload postgresql-9.6
# service_promote_command = /usr/pgsql-9.6/bin/pg_ctl -D /var/lib/pgsql/9.6/data-standby promote


# external command options

#rsync_options=--archive --checksum --compress --progress --rsh="ssh -o \"StrictHostKeyChecking no\""
#ssh_options=-o "StrictHostKeyChecking no"

# external command arguments. Values shown are examples.

#pg_ctl_options='-s'
#pg_basebackup_options='--label=repmgr_backup'

# This is the host name of the barman server, which is used for connecting over
# to the barman server (passwordless ssh keys should be in place)
#barman_server='backup_server'
# If you are placing the barman.conf file in a non-standard path, or using
# a name other than barman.conf, use this parameter to specify the path and
# name of the barman configuration file.
#barman_config='/path/to/barman.conf'

# Standby clone settings
# ----------------------
#
# These settings apply when cloning a standby (`repmgr standby clone`).

# Tablespaces can be remapped from one file system location to another:
#
# tablespace_mapping=/path/to/original/tablespace=/path/to/new/tablespace

# You can specify a restore_command to be used in the recovery.conf that
# will be placed in the cloned standby
#
# restore_command = cp /path/to/archived/wals/%f %p

# Failover settings (repmgrd)
# ---------------------------
#
# These settings are only applied when repmgrd is running. Values shown
# are defaults.

# monitoring interval in seconds; default is 2
#monitor_interval_secs=2

# Maximum number of seconds to wait for a response from the primary server
# before deciding it has failed.
master_response_timeout=5

# Number of attempts at what interval (in seconds) to try and
# connect to  a server to establish its status (e.g. master
# during failover)
reconnect_attempts=3
reconnect_interval=5

# Autofailover options
#failover=manual    # one of 'automatic', 'manual' (default: manual)
                    # defines the action to take in the event of upstream failure
                    #
                    # 'automatic': repmgrd will automatically attempt to promote the
                    #    node or follow the new upstream node
                    # 'manual': repmgrd will take no action and the mode will require
                    #    manual attention to reattach it to replication

#priority=100       # indicate a preferred priorty for promoting nodes
                    # a value of zero or less prevents the node being promoted to primary
                    # (default: 100)

#promote_command='/usr/pgsql-9.6/bin/repmgr standby promote -f /etc/repmgr/9.6/repmgr.conf'
#follow_command='/usr/pgsql-9.6/bin/repmgr standby follow -f /etc/repmgr/9.6/repmgr.conf'

# change wait time for primary; before we bail out and exit when the primary
# disappears, we wait 'reconnect_attempts' * 'retry_promote_interval_secs'
# seconds; by default this would be half an hour, as 'retry_promote_interval_secs'
# default value is 300)
retry_promote_interval_secs=10

# Number of seconds after which the witness server resyncs the repl_nodes table
#witness_repl_nodes_sync_interval_secs=15


~~~~
